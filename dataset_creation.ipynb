{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b28af",
   "metadata": {},
   "source": [
    "DATASET CREATION (Bacterial Genomes to Labeled Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a775bb",
   "metadata": {},
   "source": [
    "## Notebook dependency helper\n",
    "\n",
    "If this notebook raises ModuleNotFoundError for modules like `pandas` or `Bio`, run the cell below to install them into the active kernel. This installs into the same Python that the kernel is using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell installs missing dependencies into the active kernel; run this cell once before the main script if you see ModuleNotFoundError\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "missing = []\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    missing.append(\"pandas\")\n",
    "try:\n",
    "    from Bio import SeqIO\n",
    "except Exception:\n",
    "    missing.append(\"biopython\")\n",
    "\n",
    "if missing:\n",
    "    print('Installing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade'] + missing)\n",
    "    print('Installation finished â€” restart the kernel or re-run the notebook cells if necessary.')\n",
    "else:\n",
    "    print('All required packages are already available in the kernel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3f759e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeqIO\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "#   DNA DATASET CREATION SCRIPT\n",
    "#   Multiclass (4 classes)\n",
    "#   15,000 samples per class\n",
    "#   Sequence length = 300 bp\n",
    "#   Output: CSV files\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "GENOMES_DIR = \"data/genomes/\"          # Folder containing .gbff/.gbk files\n",
    "SEQ_LEN = 300                          # Fixed window length\n",
    "SAMPLES_PER_CLASS = 15000              # Per class\n",
    "CLASSES = [\"promoter\", \"cds\", \"terminator\", \"intergenic\"]\n",
    "\n",
    "# ==========================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================================\n",
    "\n",
    "def extract_promoters(record, flanking=150):\n",
    "    \"\"\"Extract promoter regions based on gene annotations.\"\"\"\n",
    "    promoters = []\n",
    "    for feature in record.features:\n",
    "        if feature.type == \"gene\" or feature.type == \"CDS\":\n",
    "            try:\n",
    "                start = int(feature.location.start)\n",
    "                end   = int(feature.location.end)\n",
    "                strand = feature.location.strand\n",
    "\n",
    "                if strand == 1:\n",
    "                    prom_start = max(0, start - flanking)\n",
    "                    prom_end   = start + 50\n",
    "                else:\n",
    "                    prom_start = max(0, end - 50)\n",
    "                    prom_end   = min(len(record.seq), end + flanking)\n",
    "\n",
    "                promoters.append(str(record.seq[prom_start:prom_end]))\n",
    "            except:\n",
    "                pass\n",
    "    return promoters\n",
    "\n",
    "\n",
    "def extract_cds(record):\n",
    "    cds_list = []\n",
    "    for f in record.features:\n",
    "        if f.type == \"CDS\":\n",
    "            try:\n",
    "                seq = f.extract(record.seq)\n",
    "                cds_list.append(str(seq))\n",
    "            except:\n",
    "                pass\n",
    "    return cds_list\n",
    "\n",
    "\n",
    "def extract_terminators(record):\n",
    "    \"\"\"Simplified terminator extraction: near poly-T or hairpin-like regions.\"\"\"\n",
    "    seq = str(record.seq)\n",
    "    terminators = []\n",
    "    \n",
    "    for i in range(0, len(seq)-40, 40):\n",
    "        window = seq[i:i+40]\n",
    "        if window.count(\"T\") > 20:  # crude terminator signal (poly-T tail)\n",
    "            terminators.append(seq[i:i+SEQ_LEN])\n",
    "    return terminators\n",
    "\n",
    "\n",
    "def extract_intergenic(record):\n",
    "    inter = []\n",
    "    seq = str(record.seq)\n",
    "    occupied = []\n",
    "\n",
    "    for f in record.features:\n",
    "        if \"location\" in f.__dict__:\n",
    "            s = int(f.location.start)\n",
    "            e = int(f.location.end)\n",
    "            occupied.append((s, e))\n",
    "\n",
    "    occupied = sorted(occupied)\n",
    "    intergenic_regions = []\n",
    "\n",
    "    # regions between genes\n",
    "    last_end = 0\n",
    "    for (s, e) in occupied:\n",
    "        if s - last_end > SEQ_LEN:\n",
    "            intergenic_regions.append(seq[last_end:s])\n",
    "        last_end = e\n",
    "\n",
    "    # convert long regions into windows\n",
    "    windows = []\n",
    "    for region in intergenic_regions:\n",
    "        for i in range(0, len(region) - SEQ_LEN, 100):\n",
    "            windows.append(region[i:i+SEQ_LEN])\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def clean_and_fix_length(seq, length=300):\n",
    "    seq = seq.upper()\n",
    "    seq = seq.replace(\"N\", \"A\")  # replace ambiguous bases\n",
    "    if len(seq) < length:\n",
    "        return None\n",
    "    return seq[:length]\n",
    "\n",
    "\n",
    "def kmerize(seq, k=3):\n",
    "    return \" \".join([seq[i:i+k] for i in range(len(seq)-k+1)])\n",
    "\n",
    "\n",
    "def tokenize_dl(seq):\n",
    "    mapping = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "    return [mapping.get(b, 0) for b in seq]\n",
    "    \n",
    "\n",
    "# ==========================================================\n",
    "# MAIN EXTRACTION LOOP\n",
    "# ==========================================================\n",
    "all_promoters = []\n",
    "all_cds = []\n",
    "all_terminators = []\n",
    "all_intergenics = []\n",
    "\n",
    "print(\"Reading genomes from:\", GENOMES_DIR)\n",
    "files = [f for f in os.listdir(GENOMES_DIR) if f.endswith(\".gbff\") or f.endswith(\".gbk\")]\n",
    "\n",
    "for file in files:\n",
    "    print(\"Processing genome:\", file)\n",
    "    path = os.path.join(GENOMES_DIR, file)\n",
    "    for record in SeqIO.parse(path, \"genbank\"):\n",
    "        \n",
    "        # Promoters\n",
    "        proms = extract_promoters(record)\n",
    "        for p in proms:\n",
    "            p = clean_and_fix_length(p, SEQ_LEN)\n",
    "            if p: all_promoters.append(p)\n",
    "\n",
    "        # CDS\n",
    "        cds_list = extract_cds(record)\n",
    "        for c in cds_list:\n",
    "            c = clean_and_fix_length(c, SEQ_LEN)\n",
    "            if c: all_cds.append(c)\n",
    "\n",
    "        # Terminators\n",
    "        terms = extract_terminators(record)\n",
    "        for t in terms:\n",
    "            t = clean_and_fix_length(t, SEQ_LEN)\n",
    "            if t: all_terminators.append(t)\n",
    "\n",
    "        # Intergenic\n",
    "        inter = extract_intergenic(record)\n",
    "        for i in inter:\n",
    "            i = clean_and_fix_length(i, SEQ_LEN)\n",
    "            if i: all_intergenics.append(i)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# BALANCE & SAMPLE CLASSES\n",
    "# ==========================================================\n",
    "print(\"Sampling 15,000 per class...\")\n",
    "\n",
    "promoters = random.sample(all_promoters, SAMPLES_PER_CLASS)\n",
    "cds       = random.sample(all_cds, SAMPLES_PER_CLASS)\n",
    "terms     = random.sample(all_terminators, SAMPLES_PER_CLASS)\n",
    "inter     = random.sample(all_intergenics, SAMPLES_PER_CLASS)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "def add_samples(seqs, label):\n",
    "    for s in seqs:\n",
    "        dataset.append([s, label])\n",
    "\n",
    "add_samples(promoters, \"promoter\")\n",
    "add_samples(cds, \"cds\")\n",
    "add_samples(terms, \"terminator\")\n",
    "add_samples(inter, \"intergenic\")\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "df_raw = pd.DataFrame(dataset, columns=[\"sequence\", \"label\"])\n",
    "\n",
    "# ==========================================================\n",
    "# SAVE RAW\n",
    "# ==========================================================\n",
    "df_raw.to_csv(\"dataset_raw.csv\", index=False)\n",
    "print(\"Saved: dataset_raw.csv\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# GENERATE ML VERSION (k-mer 3)\n",
    "# ==========================================================\n",
    "df_ml = df_raw.copy()\n",
    "df_ml[\"kmers\"] = df_raw[\"sequence\"].apply(lambda x: kmerize(x, k=3))\n",
    "df_ml.to_csv(\"dataset_ml.csv\", index=False)\n",
    "print(\"Saved: dataset_ml.csv (Naive Bayes + N-gram LM)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# GENERATE DL VERSION (integer tokens)\n",
    "# ==========================================================\n",
    "df_dl = df_raw.copy()\n",
    "df_dl[\"tokens\"] = df_raw[\"sequence\"].apply(tokenize_dl)\n",
    "df_dl.to_csv(\"dataset_dl.csv\", index=False)\n",
    "print(\"Saved: dataset_dl.csv (RNN / BiLSTM / Transformer)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# SUMMARY\n",
    "# ==========================================================\n",
    "print(\"\\nDataset creation complete!\")\n",
    "print(\"Total samples:\", len(df_raw))\n",
    "print(df_raw[\"label\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
