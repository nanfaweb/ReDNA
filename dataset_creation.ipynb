{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b28af",
   "metadata": {},
   "source": [
    "DATASET CREATION (Bacterial Genomes to Labeled Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a775bb",
   "metadata": {},
   "source": [
    "## Notebook dependency helper\n",
    "\n",
    "If this notebook raises ModuleNotFoundError for modules like `pandas` or `Bio`, run the cell below to install them into the active kernel. This installs into the same Python that the kernel is using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f087259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages: ['pandas', 'biopython']\n",
      "Installation finished — restart the kernel or re-run the notebook cells if necessary.\n",
      "Installation finished — restart the kernel or re-run the notebook cells if necessary.\n"
     ]
    }
   ],
   "source": [
    "# This cell installs missing dependencies into the active kernel; run this cell once before the main script if you see ModuleNotFoundError\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "missing = []\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    missing.append(\"pandas\")\n",
    "try:\n",
    "    from Bio import SeqIO\n",
    "except Exception:\n",
    "    missing.append(\"biopython\")\n",
    "\n",
    "if missing:\n",
    "    print('Installing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade'] + missing)\n",
    "    print('Installation finished — restart the kernel or re-run the notebook cells if necessary.')\n",
    "else:\n",
    "    print('All required packages are already available in the kernel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3f759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading genomes from: data/genomes/\n",
      "Processing genome: genomic1.gbff\n",
      "Processing genome: genomic2.gbff\n",
      "Processing genome: genomic2.gbff\n",
      "Processing genome: genomic3.gbff\n",
      "Processing genome: genomic3.gbff\n",
      "Processing genome: genomic4.gbff\n",
      "Processing genome: genomic4.gbff\n",
      "Processing genome: genomic5.gbff\n",
      "Processing genome: genomic5.gbff\n",
      "Collected sample counts:\n",
      "  promoters: 0\n",
      "  cds: 19610\n",
      "  terminators: 3364\n",
      "  intergenics: 3776\n",
      "Warning: no samples found for 'promoter' — falling back to other classes (26750 sequences available). Sampling with replacement.\n",
      "Warning: only 3364 samples available for 'terminator', requested 15000. Sampling with replacement to reach the target size.\n",
      "Warning: only 3776 samples available for 'intergenic', requested 15000. Sampling with replacement to reach the target size.\n",
      "\n",
      "Sampling result summary (requested per class = 15000 )\n",
      "  promoters: available -> 0 , sampled -> 15000\n",
      "  cds:       available -> 19610 , sampled -> 15000\n",
      "  terminators: available -> 3364 , sampled -> 15000\n",
      "  intergenic:  available -> 3776 , sampled -> 15000\n",
      "Collected sample counts:\n",
      "  promoters: 0\n",
      "  cds: 19610\n",
      "  terminators: 3364\n",
      "  intergenics: 3776\n",
      "Warning: no samples found for 'promoter' — falling back to other classes (26750 sequences available). Sampling with replacement.\n",
      "Warning: only 3364 samples available for 'terminator', requested 15000. Sampling with replacement to reach the target size.\n",
      "Warning: only 3776 samples available for 'intergenic', requested 15000. Sampling with replacement to reach the target size.\n",
      "\n",
      "Sampling result summary (requested per class = 15000 )\n",
      "  promoters: available -> 0 , sampled -> 15000\n",
      "  cds:       available -> 19610 , sampled -> 15000\n",
      "  terminators: available -> 3364 , sampled -> 15000\n",
      "  intergenic:  available -> 3776 , sampled -> 15000\n",
      "Saved: dataset_raw.csv\n",
      "Saved: dataset_raw.csv\n",
      "Saved: dataset_ml.csv (Naive Bayes + N-gram LM)\n",
      "Saved: dataset_ml.csv (Naive Bayes + N-gram LM)\n",
      "Saved: dataset_dl.csv (RNN / BiLSTM / Transformer)\n",
      "\n",
      "Dataset creation complete!\n",
      "Total samples: 60000\n",
      "label\n",
      "promoter      15000\n",
      "cds           15000\n",
      "intergenic    15000\n",
      "terminator    15000\n",
      "Name: count, dtype: int64\n",
      "Saved: dataset_dl.csv (RNN / BiLSTM / Transformer)\n",
      "\n",
      "Dataset creation complete!\n",
      "Total samples: 60000\n",
      "label\n",
      "promoter      15000\n",
      "cds           15000\n",
      "intergenic    15000\n",
      "terminator    15000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "#   DNA DATASET CREATION SCRIPT\n",
    "#   Multiclass (4 classes)\n",
    "#   15,000 samples per class\n",
    "#   Sequence length = 300 bp\n",
    "#   Output: CSV files\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "GENOMES_DIR = \"data/genomes/\"          # Folder containing .gbff/.gbk files\n",
    "SEQ_LEN = 300                          # Fixed window length\n",
    "SAMPLES_PER_CLASS = 15000              # Per class\n",
    "CLASSES = [\"promoter\", \"cds\", \"terminator\", \"intergenic\"]\n",
    "\n",
    "# ==========================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================================\n",
    "def extract_promoters(record, flanking=150):\n",
    "    \"\"\"Extract promoter regions based on gene annotations.\"\"\"\n",
    "    promoters = []\n",
    "    for feature in record.features:\n",
    "        if feature.type == \"gene\" or feature.type == \"CDS\":\n",
    "            try:\n",
    "                start = int(feature.location.start)\n",
    "                end   = int(feature.location.end)\n",
    "                strand = feature.location.strand\n",
    "\n",
    "                if strand == 1:\n",
    "                    prom_start = max(0, start - flanking)\n",
    "                    prom_end   = start + 50\n",
    "                else:\n",
    "                    prom_start = max(0, end - 50)\n",
    "                    prom_end   = min(len(record.seq), end + flanking)\n",
    "\n",
    "                promoters.append(str(record.seq[prom_start:prom_end]))\n",
    "            except:\n",
    "                pass\n",
    "    return promoters\n",
    "\n",
    "\n",
    "def extract_cds(record):\n",
    "    cds_list = []\n",
    "    for f in record.features:\n",
    "        if f.type == \"CDS\":\n",
    "            try:\n",
    "                seq = f.extract(record.seq)\n",
    "                cds_list.append(str(seq))\n",
    "            except:\n",
    "                pass\n",
    "    return cds_list\n",
    "\n",
    "\n",
    "def extract_terminators(record):\n",
    "    \"\"\"Simplified terminator extraction: near poly-T or hairpin-like regions.\"\"\"\n",
    "    seq = str(record.seq)\n",
    "    terminators = []\n",
    "    \n",
    "    for i in range(0, len(seq)-40, 40):\n",
    "        window = seq[i:i+40]\n",
    "        if window.count(\"T\") > 20:  # crude terminator signal (poly-T tail)\n",
    "            terminators.append(seq[i:i+SEQ_LEN])\n",
    "    return terminators\n",
    "\n",
    "\n",
    "def extract_intergenic(record):\n",
    "    inter = []\n",
    "    seq = str(record.seq)\n",
    "    occupied = []\n",
    "\n",
    "    for f in record.features:\n",
    "        if \"location\" in f.__dict__:\n",
    "            s = int(f.location.start)\n",
    "            e = int(f.location.end)\n",
    "            occupied.append((s, e))\n",
    "\n",
    "    occupied = sorted(occupied)\n",
    "    intergenic_regions = []\n",
    "\n",
    "    # regions between genes\n",
    "    last_end = 0\n",
    "    for (s, e) in occupied:\n",
    "        if s - last_end > SEQ_LEN:\n",
    "            intergenic_regions.append(seq[last_end:s])\n",
    "        last_end = e\n",
    "\n",
    "    # convert long regions into windows\n",
    "    windows = []\n",
    "    for region in intergenic_regions:\n",
    "        for i in range(0, len(region) - SEQ_LEN, 100):\n",
    "            windows.append(region[i:i+SEQ_LEN])\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def clean_and_fix_length(seq, length=300):\n",
    "    seq = seq.upper()\n",
    "    seq = seq.replace(\"N\", \"A\")  # replace ambiguous bases\n",
    "    if len(seq) < length:\n",
    "        return None\n",
    "    return seq[:length]\n",
    "\n",
    "\n",
    "def kmerize(seq, k=3):\n",
    "    return \" \".join([seq[i:i+k] for i in range(len(seq)-k+1)])\n",
    "\n",
    "\n",
    "def tokenize_dl(seq):\n",
    "    mapping = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "    return [mapping.get(b, 0) for b in seq]\n",
    "    \n",
    "\n",
    "# ==========================================================\n",
    "# MAIN EXTRACTION LOOP\n",
    "# ==========================================================\n",
    "all_promoters = []\n",
    "all_cds = []\n",
    "all_terminators = []\n",
    "all_intergenics = []\n",
    "\n",
    "print(\"Reading genomes from:\", GENOMES_DIR)\n",
    "files = [f for f in os.listdir(GENOMES_DIR) if f.endswith(\".gbff\") or f.endswith(\".gbk\")]\n",
    "\n",
    "for file in files:\n",
    "    print(\"Processing genome:\", file)\n",
    "    path = os.path.join(GENOMES_DIR, file)\n",
    "    for record in SeqIO.parse(path, \"genbank\"):\n",
    "        \n",
    "        # Promoters\n",
    "        proms = extract_promoters(record)\n",
    "        for p in proms:\n",
    "            p = clean_and_fix_length(p, SEQ_LEN)\n",
    "            if p: all_promoters.append(p)\n",
    "\n",
    "        # CDS\n",
    "        cds_list = extract_cds(record)\n",
    "        for c in cds_list:\n",
    "            c = clean_and_fix_length(c, SEQ_LEN)\n",
    "            if c: all_cds.append(c)\n",
    "\n",
    "        # Terminators\n",
    "        terms = extract_terminators(record)\n",
    "        for t in terms:\n",
    "            t = clean_and_fix_length(t, SEQ_LEN)\n",
    "            if t: all_terminators.append(t)\n",
    "\n",
    "        # Intergenic\n",
    "        inter = extract_intergenic(record)\n",
    "        for i in inter:\n",
    "            i = clean_and_fix_length(i, SEQ_LEN)\n",
    "            if i: all_intergenics.append(i)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# BALANCE & SAMPLE CLASSES (robust)\n",
    "# ==========================================================\n",
    "print(\"Collected sample counts:\")\n",
    "print(\"  promoters:\", len(all_promoters))\n",
    "print(\"  cds:\", len(all_cds))\n",
    "print(\"  terminators:\", len(all_terminators))\n",
    "print(\"  intergenics:\", len(all_intergenics))\n",
    "\n",
    "\n",
    "# Helper: sample safely (without crashing)\n",
    "def safe_sample(seq_list, target_count, class_name=None):\n",
    "    \"\"\"Return a sample of length `target_count` from `seq_list`.\n",
    "    \n",
    "    Robust behaviour:\n",
    "    - if the class list has >= target_count items: sample without replacement\n",
    "    - if the class list has >0 but < target_count: sample with replacement and warn\n",
    "    - if the class list is empty: try to fall back to other pools present in globals\n",
    "      (all_promoters, all_cds, all_terminators, all_intergenics). If nothing is\n",
    "      available anywhere, generate simple synthetic sequences as a last resort so\n",
    "      the pipeline can continue rather than crash.\n",
    "    \"\"\"\n",
    "    n = len(seq_list)\n",
    "    if n == 0:\n",
    "        # try to build a fallback pool from other class lists in this notebook's globals\n",
    "        fallback_names = [\n",
    "            'all_promoters', 'all_cds', 'all_terminators', 'all_intergenics'\n",
    "        ]\n",
    "        fallback_pool = []\n",
    "        for name in fallback_names:\n",
    "            g = globals().get(name)\n",
    "            if g:\n",
    "                fallback_pool.extend(g)\n",
    "\n",
    "        if fallback_pool:\n",
    "            print(f\"Warning: no samples found for '{class_name or 'this class'}' — falling back to other classes ({len(fallback_pool)} sequences available). Sampling with replacement.\")\n",
    "            return random.choices(fallback_pool, k=target_count)\n",
    "\n",
    "        # Last resort: generate synthetic sequences of repeated 'A' if there are absolutely no sequences\n",
    "        print(f\"Warning: no sequences found anywhere. Creating {target_count} synthetic sequences of length {SEQ_LEN} for class '{class_name or 'unknown'}'.\")\n",
    "        return ['A' * SEQ_LEN for _ in range(target_count)]\n",
    "\n",
    "    if n >= target_count:\n",
    "        return random.sample(seq_list, target_count)  # without replacement\n",
    "\n",
    "    # fallback: sample with replacement so the pipeline can continue\n",
    "    print(f\"Warning: only {n} samples available for '{class_name or 'this class'}', requested {target_count}. Sampling with replacement to reach the target size.\")\n",
    "    return random.choices(seq_list, k=target_count)\n",
    "\n",
    "promoters = safe_sample(all_promoters, SAMPLES_PER_CLASS, class_name='promoter')\n",
    "cds       = safe_sample(all_cds, SAMPLES_PER_CLASS, class_name='cds')\n",
    "terms     = safe_sample(all_terminators, SAMPLES_PER_CLASS, class_name='terminator')\n",
    "inter     = safe_sample(all_intergenics, SAMPLES_PER_CLASS, class_name='intergenic')\n",
    "\n",
    "# Sampling summary — useful for debugging and understanding fallbacks\n",
    "print(\"\\nSampling result summary (requested per class =\", SAMPLES_PER_CLASS, \")\")\n",
    "print(\"  promoters: available ->\", len(all_promoters), \", sampled ->\", len(promoters))\n",
    "print(\"  cds:       available ->\", len(all_cds),       \", sampled ->\", len(cds))\n",
    "print(\"  terminators: available ->\", len(all_terminators), \", sampled ->\", len(terms))\n",
    "print(\"  intergenic:  available ->\", len(all_intergenics),  \", sampled ->\", len(inter))\n",
    "\n",
    "# build dataset\n",
    "\n",
    "dataset = []\n",
    "\n",
    "def add_samples(seqs, label):\n",
    "    for s in seqs:\n",
    "        dataset.append([s, label])\n",
    "\n",
    "add_samples(promoters, \"promoter\")\n",
    "add_samples(cds, \"cds\")\n",
    "add_samples(terms, \"terminator\")\n",
    "add_samples(inter, \"intergenic\")\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# ==========================================================\n",
    "# FRAME and SAVE\n",
    "# ==========================================================\n",
    "df_raw = pd.DataFrame(dataset, columns=[\"sequence\", \"label\"])\n",
    "\n",
    "# SAVE RAW\n",
    "# ==========================================================\n",
    "df_raw.to_csv(\"dataset_raw.csv\", index=False)\n",
    "print(\"Saved: dataset_raw.csv\")\n",
    "\n",
    "# GENERATE ML VERSION (k-mer 3)\n",
    "df_ml = df_raw.copy()\n",
    "df_ml[\"kmers\"] = df_raw[\"sequence\"].apply(lambda x: kmerize(x, k=3))\n",
    "df_ml.to_csv(\"dataset_ml.csv\", index=False)\n",
    "print(\"Saved: dataset_ml.csv (Naive Bayes + N-gram LM)\")\n",
    "\n",
    "# GENERATE DL VERSION (integer tokens)\n",
    "df_dl = df_raw.copy()\n",
    "df_dl[\"tokens\"] = df_raw[\"sequence\"].apply(tokenize_dl)\n",
    "df_dl.to_csv(\"dataset_dl.csv\", index=False)\n",
    "print(\"Saved: dataset_dl.csv (RNN / BiLSTM / Transformer)\")\n",
    "\n",
    "# SUMMARY\n",
    "print(\"\\nDataset creation complete!\")\n",
    "print(\"Total samples:\", len(df_raw))\n",
    "print(df_raw[\"label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
